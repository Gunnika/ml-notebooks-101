{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron to perform Logical operations\n",
    "\n",
    "This notebook implements a basic perceptron to perform logical AND and OR operations. However this approach can be used to model all the other logical operations easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for some math utility functions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Function to compute sigmoid\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    '''\n",
    "    Function to compute cross entropy loss\n",
    "    '''\n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)\n",
    "    else:\n",
    "        return -np.log(1 - y_hat)"
   ]
  },
  {
   "source": [
    "## Logical OR Operation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPerceptron:\n",
    "    '''\n",
    "    Implements an basic artificial neuron, which is good enough for logical gates. \n",
    "    Basic Operation --> w0*x1 + w1*x2 + b \n",
    "    w --> weights \n",
    "    b --> biases\n",
    "    x --> input\n",
    "    \n",
    "    Since OR Game requires 2 inputs it is x1 and x2 given as input. \n",
    "    '''\n",
    "    def __init__(self, init_method=\"random\", thresh=0.5):\n",
    "        # initialize the weights and biases\n",
    "        self.W_0 = None\n",
    "        self.W_1 = None\n",
    "        self.b = None\n",
    "        \n",
    "        self._initialize(init_method)\n",
    "        \n",
    "    def _initialize(self, method):\n",
    "        '''\n",
    "        Helper function for weight initialization, supports 3 methods\n",
    "        \"random\" - Random weight initilization (close to zero)\n",
    "        \"zeros\"  - initializes with zeros\n",
    "        \"or_gate\"- initializes weights for OR Gate (manually computed)\n",
    "                   Formula = 2*x1 + 2*x2 + (-1)\n",
    "                   Sigmoid = g(x)\n",
    "                   When x=0,y=0 --> 2*0 + 2*0 + (-1) = g(-1) = 0\n",
    "                   When x=0,y=1 --> 2*0 + 2*1 + (-1) = g(1) = 1\n",
    "                   When x=1,y=0 --> 2*1 + 2*0 + (-1) = g(1) = 1\n",
    "                   When x=1,y=1 --> 2*1 + 2*1 + (-1) = g(3) = 1\n",
    "                    \n",
    "        ''' \n",
    "        if method == \"random\":\n",
    "            self.W_0 = (np.random.randn(1)*0.1)[0]\n",
    "            self.W_1 = (np.random.randn(1)*0.1)[0]\n",
    "            self.b = (np.random.randn(1)*0.1)[0]\n",
    "        elif method == \"or_gate\":\n",
    "            self.W_0 = 2\n",
    "            self.W_1 = 2\n",
    "            self.b = -1\n",
    "        elif method == \"zeros\":\n",
    "            self.W_0 = 0.0\n",
    "            self.W_1 = 0.0\n",
    "            self.b = 0.0\n",
    "        else:\n",
    "            raise(\"Initialization method unknown. Choose one of [random, or_gate, zeros]\")\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        '''\n",
    "        Performs one forward pass through the network to obtain the prediction\n",
    "        ''' \n",
    "        x1, x2 = X\n",
    "        return sigmoid(self.W_0 * x1 + self.W_1 * x2 + self.b)    \n",
    "    \n",
    "    def train(self, train_samples, learning_rate, epochs):\n",
    "        '''\n",
    "        Train function - Since we are training just a single neuron,\n",
    "        computing derivative and chain rule to update weights isn't necessary.\n",
    "        \n",
    "        if W and b are weight matrices, the operation performed here is,\n",
    "           W = W + lr * (y - y_hat) * X\n",
    "           b = b + lr * (y - y_hat)\n",
    "           \n",
    "        This simple strategy is good enough for this problem\n",
    "        '''\n",
    "        for ep in range(epochs):\n",
    "            losses = []\n",
    "            for _, (X, y) in enumerate(train_samples):\n",
    "                pred = self.feed_forward(X)\n",
    "                losses.append(cross_entropy_loss(pred, y))\n",
    "                for i in range(len(X)):\n",
    "                    self.W_0 += learning_rate * (y - pred) * X[i]\n",
    "                    self.W_1 += learning_rate * (y - pred) * X[i]\n",
    "                self.b += learning_rate * (y - pred)\n",
    "                    \n",
    "            print(f\"Epoch: {ep} --> Loss: {np.mean(losses)}\")\n",
    "            \n",
    "    def predict(self, inputs, return_prob=False):\n",
    "        '''\n",
    "        Predicts outputs for the given array of input\n",
    "        '''\n",
    "        preds = []\n",
    "        \n",
    "        for inp in inputs:\n",
    "            prob = self.feed_forward(inp)\n",
    "            if return_prob:\n",
    "                preds.append(prob)\n",
    "                continue\n",
    "            preds.append(int(prob > 0.5))\n",
    "            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pretuned values\n",
    "As mentioned in the function description above, first lets test out the manually tuned values. \n",
    "\n",
    "> **Note: This is not the only combination of weights that result in similar behaviour. w0 = 20, w1 =20 and b =-10 should also work the same way.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In: [0, 0] --> out: 0\nIn: [0, 1] --> out: 1\nIn: [1, 0] --> out: 1\nIn: [1, 1] --> out: 1\n"
     ]
    }
   ],
   "source": [
    "nn_ = BasicPerceptron(init_method=\"or_gate\")\n",
    "\n",
    "in_data = [[0, 0], [0, 1], [1, 0], [1, 1]] \n",
    "\n",
    "preds = nn_.predict(in_data, return_prob=False)\n",
    "\n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"In: {in_data[i]} --> out: {preds[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network from scratch\n",
    "Now the NN is randomly initialized and trained to make it learn the Logical OR operation itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = BasicPerceptron(init_method=\"random\")\n",
    "\n",
    "out_data = [0, 1, 1, 1]\n",
    "\n",
    "train_samples = list(zip(in_data, out_data))\n",
    "\n",
    "def generate_data(array, size):\n",
    "    train_data = []\n",
    "    for i in range(size):\n",
    "        train_data.append(array[np.random.choice(range(len(array)))])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.07526163781112757 0.1417556473765587 -0.2034419259959832\n"
     ]
    }
   ],
   "source": [
    "print(nn.W_0, nn.W_1, nn.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_data(train_samples, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0 --> Loss: 0.1880486816193276\n",
      "Epoch: 1 --> Loss: 0.06973467787108198\n",
      "Epoch: 2 --> Loss: 0.04231946738096062\n",
      "Epoch: 3 --> Loss: 0.030193460879004336\n",
      "Epoch: 4 --> Loss: 0.02340696191004151\n",
      "Epoch: 5 --> Loss: 0.019085030532672433\n",
      "Epoch: 6 --> Loss: 0.01609735568629557\n",
      "Epoch: 7 --> Loss: 0.013911343778923054\n",
      "Epoch: 8 --> Loss: 0.01224383275830612\n",
      "Epoch: 9 --> Loss: 0.01093062938887168\n"
     ]
    }
   ],
   "source": [
    "nn.train(train_data, learning_rate=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in: [0, 0] --> prob: 0.0261 --> out: 0\nin: [0, 1] --> prob: 0.9921 --> out: 1\nin: [1, 0] --> prob: 0.9915 --> out: 1\nin: [1, 1] --> prob: 1.0 --> out: 1\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(in_data, return_prob=True)\n",
    "   \n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"in: {in_data[i]} --> prob: {round(preds[i], 4)} --> out: {int(preds[i] > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_data(train_samples, 100)\n",
    "\n",
    "X_test = list(zip(*test_data))[0]\n",
    "y_test = list(zip(*test_data))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(X_test)\n",
    "\n",
    "def compute_accuracy(y_hat, y):\n",
    "    correct_counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if y_hat[i] == y[i]:\n",
    "            correct_counter += 1\n",
    "    return correct_counter / len(y)\n",
    "    \n",
    "acc = compute_accuracy(preds, y_test)\n",
    "print(f\"Test Accuracy: {acc}\")"
   ]
  },
  {
   "source": [
    "As you can see our NN now becomes really good at this simple task and gets 100% accuracy. Which in this case is not an anomaly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logical AND Operation\n",
    "\n",
    "Now instead of changing the architecture I will directly change the weights to make it perform AND Operation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In: [0, 0] --> out: 0\nIn: [0, 1] --> out: 0\nIn: [1, 0] --> out: 0\nIn: [1, 1] --> out: 1\n"
     ]
    }
   ],
   "source": [
    "nn_ = BasicPerceptron()\n",
    "\n",
    "nn_.W_0 = 2\n",
    "nn_.W_1 = 2\n",
    "nn_.b = -3\n",
    "\n",
    "in_data = [[0, 0], [0, 1], [1, 0], [1, 1]] \n",
    "\n",
    "preds = nn_.predict(in_data, return_prob=False)\n",
    "\n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"In: {in_data[i]} --> out: {preds[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = BasicPerceptron(init_method=\"random\")\n",
    "\n",
    "out_data = [0, 0, 0, 1]\n",
    "\n",
    "train_samples = list(zip(in_data, out_data))\n",
    "\n",
    "def generate_data(array, size):\n",
    "    train_data = []\n",
    "    for i in range(size):\n",
    "        train_data.append(array[np.random.choice(range(len(array)))])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial Weights:  0.027725231123128996 -0.022169526876134483 -0.05347426017650758 \n\nEpoch: 0 --> Loss: 0.33116494528930546\nEpoch: 1 --> Loss: 0.1483587791920254\nEpoch: 2 --> Loss: 0.09761577016741785\nEpoch: 3 --> Loss: 0.07250367773649413\nEpoch: 4 --> Loss: 0.057513669257483614\nEpoch: 5 --> Loss: 0.04757611992115576\nEpoch: 6 --> Loss: 0.04051951607217667\nEpoch: 7 --> Loss: 0.035257748489094444\nEpoch: 8 --> Loss: 0.031187905344145485\nEpoch: 9 --> Loss: 0.027948912447717862\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Weights: \", nn.W_0, nn.W_1, nn.b, )\n",
    "\n",
    "train_data = generate_data(train_samples, 500)\n",
    "\n",
    "nn.train(train_data, learning_rate=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in: [0, 0] --> prob: 0.0 --> out: 0\nin: [0, 1] --> prob: 0.0321 --> out: 0\nin: [1, 0] --> prob: 0.0337 --> out: 0\nin: [1, 1] --> prob: 0.9616 --> out: 1\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(in_data, return_prob=True)\n",
    "   \n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"in: {in_data[i]} --> prob: {round(preds[i], 4)} --> out: {int(preds[i] > 0.5)}\")"
   ]
  },
  {
   "source": [
    "Again the NN easily learns to predict the right output. \n",
    "\n",
    "Thanks for reading!\n",
    "\n",
    "Author: [abhinand5](https://github.com/abhinand5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron to perform NOT operation\n",
    "\n",
    "This notebook implements a basic perceptron to perform logical NOT operation. However, this strategy can be followed for any other logical operation as well, because of the simplicity of the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for some math utility functions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Function to compute sigmoid\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    '''\n",
    "    Function to compute cross entropy loss\n",
    "    '''\n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)\n",
    "    else:\n",
    "        return -np.log(1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPerceptron:\n",
    "    '''\n",
    "    Implements an basic artificial neuron, which is good enough for logical gates. \n",
    "    Basic Operation --> w0*x1 + b \n",
    "    w --> weights \n",
    "    b --> biases\n",
    "    x --> input\n",
    "    \n",
    "    Since NOT Gate requires only 1 input it is only x1 \n",
    "    '''\n",
    "    def __init__(self, init_method=\"random\", thresh=0.5):\n",
    "        # initialize the weights and biases\n",
    "        self.W_0 = None\n",
    "        self.b = None\n",
    "        \n",
    "        self._initialize(init_method)\n",
    "        \n",
    "    def _initialize(self, method):\n",
    "        '''\n",
    "        Helper function for weight initialization, supports 3 methods\n",
    "        \"random\" - Random weight initilization (close to zero)\n",
    "        \"zeros\"  - initializes with zeros\n",
    "        \"not_gate\"- initializes weights for NOT Gate (manually computed)\n",
    "                   Formula = (-1)*x1 + 1\n",
    "                   Sigmoid = g(x)\n",
    "                   When x=0 --> (-1)*0 + 1 = g(1) = 1\n",
    "                   When x=1 --> (-1)*1 + 1 = g(0) = 0\n",
    "        ''' \n",
    "        if method == \"random\":\n",
    "            self.W_0 = (np.random.randn(1)*0.1)[0]\n",
    "            self.b = (np.random.randn(1)*0.1)[0]\n",
    "        elif method == \"not_gate\":\n",
    "            self.W_0 = -1\n",
    "            self.b = 1\n",
    "        elif method == \"zeros\":\n",
    "            self.W_0 = 0.0\n",
    "            self.b = 0.0\n",
    "        else:\n",
    "            raise(\"Initialization method unknown. Choose one of [random, not_gate, zeros]\")\n",
    "        \n",
    "    def feed_forward(self, X):\n",
    "        '''\n",
    "        Performs one forward pass through the network to obtain the prediction\n",
    "        ''' \n",
    "        return sigmoid(self.W_0 * X + self.b)    \n",
    "    \n",
    "    def train(self, train_samples, learning_rate, epochs):\n",
    "        '''\n",
    "        Train function - Since we are training just a single neuron,\n",
    "        computing derivative and chain rule to update weights isn't necessary.\n",
    "        \n",
    "        if W and b are weight matrices, the operation performed here is,\n",
    "           W = W + lr * (y - y_hat) * X\n",
    "           b = b + lr * (y - y_hat)\n",
    "           \n",
    "        This simple strategy is good enough for this problem\n",
    "        '''\n",
    "        for ep in range(epochs):\n",
    "            losses = []\n",
    "            for _, (X, y) in enumerate(train_samples):\n",
    "                pred = self.feed_forward(X)\n",
    "                losses.append(cross_entropy_loss(pred, y))\n",
    "                self.W_0 += learning_rate * (y - pred) * X\n",
    "                self.b += learning_rate * (y - pred)\n",
    "                    \n",
    "            print(f\"Epoch: {ep} --> Loss: {np.mean(losses)}\")\n",
    "            \n",
    "    def predict(self, inputs, return_prob=False):\n",
    "        '''\n",
    "        Predicts outputs for the given array of input\n",
    "        '''\n",
    "        preds = []\n",
    "        \n",
    "        for inp in inputs:\n",
    "            prob = self.feed_forward(inp)\n",
    "            if return_prob:\n",
    "                preds.append(prob)\n",
    "                continue\n",
    "            preds.append(int(prob > 0.5))\n",
    "            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pretuned values\n",
    "As mentioned in the function description above, first lets test out the manually tuned values. \n",
    "\n",
    "> **Note: This is not the only combination of weights that result in similar behaviour. w0 = 20, w1 =20 and b =-10 should also work the same way.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In: 0 --> out: 1\n",
      "In: 1 --> out: 0\n"
     ]
    }
   ],
   "source": [
    "nn_ = BasicPerceptron(init_method=\"not_gate\")\n",
    "\n",
    "in_data = [0, 1] \n",
    "\n",
    "preds = nn_.predict(in_data, return_prob=False)\n",
    "\n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"In: {in_data[i]} --> out: {preds[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network from scratch\n",
    "Now the NN is randomly initialized and trained to make it learn the Logical NOT operation itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Train Samples: [(0, 1), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "nn = BasicPerceptron(init_method=\"random\")\n",
    "\n",
    "out_data = [1, 0]\n",
    "\n",
    "train_samples = list(zip(in_data, out_data))\n",
    "\n",
    "print(f\"Unique Train Samples: {train_samples}\")\n",
    "\n",
    "def generate_data(array, size):\n",
    "    train_data = []\n",
    "    for i in range(size):\n",
    "        train_data.append(array[np.random.choice(range(len(array)))])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.015849767288848553 -0.004182081486680271\n"
     ]
    }
   ],
   "source": [
    "print(nn.W_0, nn.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_data(train_samples, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --> Loss: 0.5124459372007211\n",
      "Epoch: 1 --> Loss: 0.29921016743139073\n",
      "Epoch: 2 --> Loss: 0.2025309584983299\n",
      "Epoch: 3 --> Loss: 0.1505611393655648\n",
      "Epoch: 4 --> Loss: 0.118896221776504\n",
      "Epoch: 5 --> Loss: 0.09782994896269134\n",
      "Epoch: 6 --> Loss: 0.08290128523309746\n",
      "Epoch: 7 --> Loss: 0.07181296627505708\n",
      "Epoch: 8 --> Loss: 0.06327404410042228\n",
      "Epoch: 9 --> Loss: 0.05650793088732609\n"
     ]
    }
   ],
   "source": [
    "nn.train(train_data, learning_rate=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 0 --> prob: 0.9444 --> out: 1\n",
      "in: 1 --> prob: 0.0472 --> out: 0\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(in_data, return_prob=True)\n",
    "   \n",
    "for i, inp in enumerate(preds):\n",
    "    print(f\"in: {in_data[i]} --> prob: {round(preds[i], 4)} --> out: {int(preds[i] > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_data(train_samples, 100)\n",
    "\n",
    "X_test = list(zip(*test_data))[0]\n",
    "y_test = list(zip(*test_data))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(X_test)\n",
    "\n",
    "def compute_accuracy(y_hat, y):\n",
    "    correct_counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if y_hat[i] == y[i]:\n",
    "            correct_counter += 1\n",
    "    return correct_counter / len(y)\n",
    "    \n",
    "acc = compute_accuracy(preds, y_test)\n",
    "print(f\"Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our NN now becomes really good at this simple task and gets 100% accuracy. Which in this case is not an anomaly.\n",
    "\n",
    "**Author: [abhinand5](www.github.com/abhinand5)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch_cpu] *",
   "language": "python",
   "name": "conda-env-.conda-torch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}